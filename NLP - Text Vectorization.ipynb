{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e614bd5b",
   "metadata": {},
   "source": [
    "# Case Study: Book Recommendations from Charles Darwin\n",
    "### Data\n",
    "Charles Darwin is the most famous scientist in the world. He \n",
    "wrote many other books on a wide range of topics, including \n",
    "geology, plants or his personal life. In this project, we will \n",
    "develop a content-based book recommendation system, \n",
    "which will determine which books are close to each other based on \n",
    "how similar the discussed topics are. Let’s take a look at the books \n",
    "we will use later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88b45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob  # glob is a general term used to define techniques to match specified patterns according to rules related to Unix shell. \n",
    "folder = \"datasets/\" \n",
    "files = glob.glob(folder + \"*.txt\") \n",
    "files.sort() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c801a8",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "As the first step, we need to load the content of each book and \n",
    "check the regular expression to facilitate the process by \n",
    "removing the all non-alpha-numeric characters. We call such a \n",
    "collection of texts a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a99a23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, os \n",
    "txts = [] \n",
    "titles = [] \n",
    " \n",
    "for n in files: \n",
    "    f = open(n, encoding='utf-8-sig') \n",
    "    data = re.sub('[\\W_]+', ' ', f.read())  \n",
    "    txts.append(data) \n",
    "    titles.append(os.path.basename(n).replace('.txt', '')) \n",
    "[len(t) for t in txts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7235d1fc",
   "metadata": {},
   "source": [
    "And then, for consistency, we will refer to Darwin’s most famous \n",
    "book “On the Origin of Species” to check the results for other given \n",
    "book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a6c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(titles)): \n",
    "    if titles[i] == 'OriginofSpecies': \n",
    "        ori = i \n",
    "        print(ori)      # Index = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b92c3",
   "metadata": {},
   "source": [
    "Next step, we transform the corpus into a format by \n",
    "doing tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a6500e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m txts_split \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m txts] \n\u001b[0;32m      6\u001b[0m texts \u001b[38;5;241m=\u001b[39m [[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m txt \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stoplist] \u001b[38;5;28;01mfor\u001b[39;00m txt \u001b[38;5;129;01min\u001b[39;00m txts_split] \n\u001b[1;32m----> 7\u001b[0m \u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m20\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "stoplist = set('for a of the and to in to be which some is at that we i who whom show via may my our might as well'.split()) \n",
    " \n",
    "txts_lower_case = [i.lower() for i in txts] \n",
    "txts_split = [i.split() for i in txts] \n",
    " \n",
    "texts = [[word for word in txt if word not in stoplist] for txt in txts_split] \n",
    "texts[15][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d2fed6",
   "metadata": {},
   "source": [
    "For the next parts of text preprocessing, we use \n",
    "a stemming process, which will group together the inflected \n",
    "forms of a word so they can be analyzed as a single item: the stem. \n",
    "In order to make the process faster, we will directly load the final \n",
    "results from a pickle file and review the method used to generate \n",
    "it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d444023",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/texts_stem.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \n\u001b[1;32m----> 2\u001b[0m texts_stem \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets/texts_stem.p\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m) \n\u001b[0;32m      3\u001b[0m texts_stem[\u001b[38;5;241m15\u001b[39m][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m20\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/texts_stem.p'"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "texts_stem = pickle.load(open('datasets/texts_stem.p', 'rb')) \n",
    "texts_stem[15][0:20] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412a9e6",
   "metadata": {},
   "source": [
    "## Text Vectorization\n",
    "### Bag-of-Words Models (BoW)\n",
    "First, we need to create a universe of all words contained in our \n",
    "corpus of Charles Darwin’s books, which we call a dictionary. \n",
    "Then, using the stemmed tokens and the dictionary, we will create \n",
    "bag-of-words models (BoW) to represent our books as a list of all \n",
    "uniques tokens they contain associated with their respective \n",
    "number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "188850ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts_stem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corpora \n\u001b[1;32m----> 2\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(\u001b[43mtexts_stem\u001b[49m) \n\u001b[0;32m      3\u001b[0m bows \u001b[38;5;241m=\u001b[39m [dictionary\u001b[38;5;241m.\u001b[39mdoc2bow(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts_stem] \n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(bows[\u001b[38;5;241m15\u001b[39m][:\u001b[38;5;241m5\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'texts_stem' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim import corpora \n",
    "dictionary = corpora.Dictionary(texts_stem) \n",
    "bows = [dictionary.doc2bow(text) for text in texts_stem] \n",
    "print(bows[15][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c08fd",
   "metadata": {},
   "source": [
    "In order to better understand the model, we will transform it into \n",
    "a DataFrame and display the 10 most common stems for the book \n",
    "“On the Origin of Species”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349716ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow_origin = pd.DataFrame() \n",
    "df_bow_origin['index'] = [i[0] for i in bows[15] if i] \n",
    "df_bow_origin['occurrences'] = [i[1] for i in bows[15] if i] \n",
    "df_bow_origin['token'] = [dictionary[index] for index in \n",
    "df_bow_origin['index']] \n",
    "df_bow_origin.occurrences.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ec30b7",
   "metadata": {},
   "source": [
    "### TF-IDF Model\n",
    "Next, we will use a TF-IDF model to define the importance of \n",
    "each word depending on how frequent it is in the text. As a result, \n",
    "a high TF-IDF score for a word will indicate that this word is \n",
    "specific to this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2101c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel \n",
    "model = TfidfModel(bows) \n",
    "model[bows[15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b4dca",
   "metadata": {},
   "source": [
    "Once again, in order to better understand the model, we will \n",
    "transform it into a DataFrame and display the 10 most specific \n",
    "words for the “On the Origin of Species” book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame() \n",
    "df_tfidf['id'] = [i[0] for i in model[bows[15]]] \n",
    "df_tfidf['score'] = [i[1] for i in model[bows[15]]] \n",
    "df_tfidf['token'] = [dictionary[index] for index in df_tfidf['id']] \n",
    "df_tfidf.score.sort_values(ascending=False).head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a8887f",
   "metadata": {},
   "source": [
    "### Recommendation\n",
    "Now that we have a TF-IDF model on how specific they are to each \n",
    "book, we can measure how related to books are between each \n",
    "other. Therefore, we will use Cosine Similarity and visualize the \n",
    "results as a distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities \n",
    "\n",
    "sims = similarities.MatrixSimilarity(model[bows]) \n",
    "sim_df = pd.DataFrame(list(sims)) \n",
    "sim_df.columns = titles  \n",
    "sim_df.index = titles \n",
    "sim_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd48110",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "We now have a matrix containing all the similarity measures \n",
    "between any pair of books from Charles Darwin! We can use \n",
    "barh() to display a horizontal bar plot for which books are the \n",
    "most similar to “On the Origin of Species.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    " \n",
    "v = sim_df['OriginofSpecies'] \n",
    "v_sorted = v.sort_values() \n",
    "v_sorted.plot.barh() \n",
    "plt.xlabel('Similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde738ec",
   "metadata": {},
   "source": [
    "However, we want to have a better understanding of the big \n",
    "picture and see how Darwin’s books are generally related to each \n",
    "other. To this purpose, we will represent the whole similarity \n",
    "matrix as a dendrogram, which is a standard tool to display such \n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy \n",
    " \n",
    "Z = hierarchy.linkage(sim_df, 'ward') \n",
    "chart = hierarchy.dendrogram(Z, leaf_font_size=8, \n",
    "labels=sim_df.index, orientation=\"left\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919bfba6",
   "metadata": {},
   "source": [
    "Finally, based on the chart we created before, we can conclude that \n",
    "“the variation of animals and plants under domestication” is \n",
    "most related to “On the Origin of Species.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ec417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
